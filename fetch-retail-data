import os
import requests
from bs4 import BeautifulSoup

# URL of the GitHub directory page
url = 'https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/retail-data/by-day/'

# Directory to save CSV files
save_dir = 'by-day'

# Create the directory if it doesn't exist
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

# Base URL to fetch raw content from GitHub
base_url = 'https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/'

# Fetch the page content
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# Find all links to the CSV files in the directory
for link in soup.find_all('a', href=True):
    if link['href'].endswith('.csv'):  # Filter for CSV links
        csv_file = link['href'].split('/')[-1]  # Extract file name
        csv_url = base_url + csv_file  # Construct raw file URL
        
        print(f'Downloading {csv_file}...')
        # Download the file
        response = requests.get(csv_url)
        
        # Save the file locally
        with open(os.path.join(save_dir, csv_file), 'wb') as file:
            file.write(response.content)
        
        print(f'{csv_file} saved.')

print('Download complete.')
